[{"id": "tabby-starcoder-1b", "serviceType": "binary", "version": "1", "name": "Tabby StarCoder 1B", "description": "", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\n## \ud83d\udcbb Hardware Requirements\n\n## \ud83d\udcd2 Example Usage\n\n<img width=\"463\" alt=\"Screenshot 2023-08-01 at 22 46 39\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/04188ee4-9fbc-4d2d-9c3a-2aca359a92e3\">\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\udc2f Getting Started with Tabby VSCode Extension\n\n1. Install Tabby VSCode Extension <a href='https://marketplace.visualstudio.com/items?itemName=TabbyML.vscode-tabby' target='_blank'>here</a>\n\n<img width=\"1207\" alt=\"Screenshot 2023-08-01 at 12 27 29\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/05a85487-b2ad-4bcb-89fb-d610398b2f72\">\n\n2. Enable the extension and provide the URL where the service is running\n\n<img width=\"1180\" alt=\"Screenshot 2023-08-01 at 12 28 07\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/258eaa26-78cd-41a8-a4f1-c4924f0696aa\">\n\n3. Wait a few minutes\n\n> The model will be downloaded when the server starts. For this reason, you will need a few minutes before the extension starts to work properly.\n\n### \ud83d\udd0e Quality Benchmarks\n\nThe model has been evaluated on two code generation benchmarks: HumanEval and MTPB. Please refer to the <a href='https://arxiv.org/abs/2203.13474' target='_blank'>paper</a> for more details.\n\n### \ud83d\udeab Limitations and Biases\n\n\n## \ud83d\udcdc License\n\nThe model is under the 3-Clause BSD license.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/binary-only/coder-tabby-starcoder-1b/logo.svg", "modelInfo": {"memoryRequirements": 3204}, "interfaces": ["coder"], "defaultPort": 8000, "defaultExternalPort": 8468, "weightsDirectoryUrl": "https://huggingface.co/TabbyML/StarCoder-1B/resolve/main/", "weightsFiles": ["tabby.json", "tokenizer.json", "ggml/q8_0.gguf"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/TabbyML/tabby/releases/download/v0.3.0/tabby_aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null}, "serveCommand": "tabby_aarch64-apple-darwin serve --model . --device metal --port 8468", "banner": null}, {"id": "whisper-tiny-cpp", "name": "Whisper Tiny CPP", "beta": true, "description": "", "documentation": "# Documentation\n\n## \ud83d\udccc Description\n\nWhisper Tiny is a compact version of OpenAI's Whisper model, designed for automatic speech recognition (ASR) and speech translation. Despite its smaller size, it retains the powerful capabilities of the larger models, making it suitable for applications where computational resources or storage space are limited. <a href='https://huggingface.co/openai/whisper-tiny' target='_blank'>Learn More</a>.\n\n## \ud83d\udcbb Hardware Requirements\n\nTo run the `whisper-tiny-cpp` service on Prem, you'll just need a CPU with at least 4GiB of RAM.\n\n## \ud83d\udcd2 Example Usage\n\nWhisper Tiny can be used for various tasks, including English to English transcription, French to French transcription, and French to English translation. It can also handle long-form transcription by using a chunking algorithm, allowing it to transcribe audio samples of arbitrary length.\n\n### \ud83c\udfb6 sample.wav. You can find the file [here](https://github.com/premAI-io/prem-registry/blob/main/audio-to-text-whisper-tiny/sample.wav)\n\n<img width=\"1449\" alt=\"image\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/4b879d6b-4404-47ae-b3c9-f2f5fd38ec0e\">\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\ude80 Getting Started with OpenAI Python client\n\nThe service exposes the same endpoints as OpenAI DALL-E does. You can directly use the official `openai` python library.\n\n```python\n\n!pip install openai\n\nimport openai\n\nopenai.api_base = \"http://184.105.5.51:10111/v1\"\nopenai.api_key = \"random-string\"\n\naudio_file = open(\"./sample.wav\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)\nprint(transcript)\n\n```\n\n## \ud83d\udcdc License\n\nWhisper's code and model weights are released under the MIT License.\n", "serviceType": "binary", "version": "1", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/binary-only/audio-to-text-whisper-tiny-cpp/logo.svg", "modelInfo": {"memoryRequirements": 4800}, "interfaces": ["audio-to-text"], "defaultExternalPort": 9446, "weightsDirectoryUrl": "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/", "weightsFiles": ["ggml-tiny.bin"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/tiero/whisperd/releases/download/v0.1.12/whisperd-aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null, "x86_64-unknown-linux-gnu": "https://github.com/tiero/whisperd/releases/download/v0.1.12/whisperd-x86_64-unknown-linux-gnu"}, "serveCommand": "whisperd-aarch64-apple-darwin serve  --model-path=ggml-tiny.bin --port=9446", "banner": null}, {"id": "mistral-7b-instruct", "name": "Mistral 7B Instruct", "beta": true, "description": "", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\nThe Mistral 7B Instruct is a model built by finetuning  <a href='https://huggingface.co/mistralai/Mistral-7B-v0.1' target='_blank'>Mistral-7B-v0.1</a>.\n\n## \ud83d\udcd2 Example Usage\n\nFollowing are a few example generations with `max_token=64` set.\n\n### 1\ufe0f\u20e3 Prompt: Why do I need to run machine learning models on-premise?\n\n> 1. Security and Privacy: On-premise machine learning allows organizations to maintain full control over their data, ensuring that sensitive information is not compromised during transmission or storage. This can be particularly important for industries such as healthcare or finance where data privacy regulations are strict.\\n2. Customization: On-p\n\n### 2\ufe0f\u20e3 Prompt: How do I run my models on-premise?\n\n> 1. Install the necessary software and dependencies for your model, such as Python, TensorFlow, or PyTorch.\\n2. Download or transfer your trained model to your on-premise environment.\\n3. Set up a server or cluster to run the model, either using existing infrastructure or by purchasing additional\n\n### 3\ufe0f\u20e3 Prompt: What are the limitations of running my models on-premise?\n\n> 1. Limited scalability: On-premise infrastructure may have limited capacity to handle large volumes of data and computational workloads, which can limit the scalability of your models.\\r\\n\\r\\n2. Maintenance and upgrades: You are responsible for maintaining and upgrading the hardware and software infrastructure required\n\n### 4\ufe0f\u20e3 Prompt: What are the trade-offs of deploying models on-premise I should be aware of?\n\n> 1. Cost: On-premise deployment can be more expensive than cloud-based deployment, as it requires hardware and infrastructure to be purchased and maintained.\\n2. Scalability: On-premise deployment may not be as scalable as cloud-based deployment, as it requires physical infrastructure to be added\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83e\udd9c\ud83d\udd17 Getting Started with Langchain\n\n```bash\npip install langchain openai\n```\n\nIt can be run simply using the langchain library as shown below:\n\n```python\nimport os\nfrom langchain.schema import HumanMessage\nfrom langchain.chat_models import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8447/v1\", max_tokens=128)\nmessages = [HumanMessage(content=\"Why do I need to run machine learning models on-premise?\")]\nprint(chat(messages))\n\n# output:\n```\n\nFor using it in a chat setting we recommend using a Chat Prompt Template as shown below:\n    \n```python\n\nimport os\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat_template = \"\"\"\nYou are an AI assistant in a conversational setting.\nProvide a conversational answer to any question an User asks. Be original, concise, accurate and helpful.\n===================\n\nUser: {user_message}\nAssistant:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"user_message\"],\n    template=chat_template,\n)\n\nuser_message = \"Why do I need to run machine learning models on-premise?\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8447/v1\", max_tokens=128)\nchain = LLMChain(llm=chat, prompt=prompt, verbose=True)\nprint(chain.run(user_message=user_message))\n\n```\n\n### \ud83d\udd0e Quality Benchmarks\n\n### \ud83d\udeab Limitations and Biases\n\nThe Mistral 7B Instruct model serves as a quick demonstration of how the base model can be readily fine-tuned to achieve compelling performance. It lacks any moderation mechanisms. The developers anticipate engaging with the community on methods to refine the model's adherence to guardrails, enabling its deployment in environments necessitating moderated outputs.\n\n## \ud83d\udcdc License\nIt is made available under a permissive Apache 2.0 license allowing for commercial use, without any royalties or restrictions.", "serviceType": "binary", "version": "1", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/binary-only/chat-mistral-7b-instruct/logo.svg", "modelInfo": {"memoryRequirements": 4800}, "interfaces": ["chat"], "defaultPort": 8000, "defaultExternalPort": 8447, "weightsDirectoryUrl": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/", "weightsFiles": ["mistral-7b-instruct-v0.1.Q5_0.gguf"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/premAI-io/prem-services/releases/download/v1/cht-llama-cpp-mistral-1-aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null}, "serveCommand": "cht-llama-cpp-mistral-1-aarch64-apple-darwin --model-path=mistral-7b-instruct-v0.1.Q5_0.gguf --port=8447", "banner": null}, {"id": "tabby-codellama-7B", "serviceType": "binary", "version": "1", "name": "Tabby CodeLlama 7B", "description": "", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\nCode Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 34 billion parameters. This is the repository for the base 7B version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding. Links to other models can be found in the index at the bottom.\n\n## \ud83d\udcbb Hardware Requirements\n\n\n## \ud83d\udcd2 Example Usage\n\n<img width=\"463\" alt=\"Screenshot 2023-08-01 at 22 46 39\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/04188ee4-9fbc-4d2d-9c3a-2aca359a92e3\">\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\udc2f Getting Started with Tabby VSCode Extension\n\n1. Install Tabby VSCode Extension <a href='https://marketplace.visualstudio.com/items?itemName=TabbyML.vscode-tabby' target='_blank'>here</a>\n\n<img width=\"1207\" alt=\"Screenshot 2023-08-01 at 12 27 29\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/05a85487-b2ad-4bcb-89fb-d610398b2f72\">\n\n2. Enable the extension and provide the URL where the service is running\n\n<img width=\"1180\" alt=\"Screenshot 2023-08-01 at 12 28 07\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/258eaa26-78cd-41a8-a4f1-c4924f0696aa\">\n\n3. Wait a few minutes\n\n> The model will be downloaded when the server starts. For this reason, you will need a few minutes before the extension starts to work properly.\n\n### \ud83d\udd0e Quality Benchmarks\n\nThe model has been evaluated on two code generation benchmarks: HumanEval and MTPB. Please refer to the <a href='https://arxiv.org/abs/2203.13474' target='_blank'>paper</a> for more details.\n\n### \ud83d\udeab Limitations and Biases\n\n\n## \ud83d\udcdc License\n\nThe model is under the 3-Clause BSD license.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/binary-only/coder-tabby-codellama-7b/logo.svg", "modelInfo": {"memoryRequirements": 3204}, "interfaces": ["coder"], "defaultPort": 8080, "defaultExternalPort": 8467, "weightsDirectoryUrl": "https://huggingface.co/TabbyML/CodeLlama-7B/resolve/main/", "weightsFiles": ["tabby.json", "tokenizer.json", "ggml/q8_0.gguf"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/TabbyML/tabby/releases/download/v0.3.0/tabby_aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null}, "serveCommand": "tabby_aarch64-apple-darwin serve --model . --device metal --port 8467", "banner": null}, {"id": "mistral-7b-openorca", "name": "Mistral 7B OpenOrca", "beta": true, "description": "", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\nThe Mistral 7B OpenOrca is a model built by finetuning  <a href='https://huggingface.co/mistralai/Mistral-7B-v0.1' target='_blank'>Mistral-7B-v0.1</a> on the  \n<a href='https://huggingface.co/datasets/Open-Orca/OpenOrca' target='_blank'>OpenOrca dataset</a>.\n\n\n## \ud83d\udcd2 Example Usage\n\nFollowing are a few example generations with `max_token=64` set.\n\n### 1\ufe0f\u20e3 Prompt: Why do I need to run machine learning models on-premise?\n\n> \\n\\nThere are several reasons why you might want to run machine learning models on-premise, rather than using a cloud-based solution or a third-party service. Here are some of the key benefits:\\n\\n1. Security and privacy: Running machine learning models on-premise allows you to\n\n### 2\ufe0f\u20e3 Prompt: How do I run my models on-premise?\n\n> \\n\\nTo run your models on-premise, you will need to follow these steps:\\n\\n1. Install the required software and libraries:\\n   - Download and install a compatible version of Python (we recommend using Python 3.6 or later).\\n   - Install the necessary libraries for running your\n\n### 3\ufe0f\u20e3 Prompt: What are the limitations of running my models on-premise?\n\n> \\n\\n[INST]\\n\\nThere are several limitations to consider when running your models on-premise:\\n\\n1. Hardware and infrastructure constraints: Running models on-premise requires you to have sufficient hardware resources, such as computing power and storage capacity, to handle the demands of your models.\n\n### 4\ufe0f\u20e3 Prompt: What are the trade-offs of deploying models on-premise I should be aware of?\n\n> \\n\\nThere are several trade-offs to consider when deciding whether to deploy machine learning models on-premise or in the cloud. Here's a list of some key factors:\\n\\n1. **Cost:** On-premise deployment can be more expensive, as it requires investing in hardware, software\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83e\udd9c\ud83d\udd17 Getting Started with Langchain\n\n```bash\npip install langchain openai\n```\n\nIt can be run simply using the langchain library as shown below:\n\n```python\nimport os\nfrom langchain.schema import HumanMessage\nfrom langchain.chat_models import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8446/v1\", max_tokens=128)\nmessages = [HumanMessage(content=\"Why do I need to run machine learning models on-premise?\")]\nprint(chat(messages))\n\n# output:\n```\n\nFor using it in a chat setting we recommend using a Chat Prompt Template as shown below:\n    \n```python\n\nimport os\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat_template = \"\"\"\nYou are an AI assistant in a conversational setting.\nProvide a conversational answer to any question an User asks. Be original, concise, accurate and helpful.\n===================\n\nUser: {user_message}\nAssistant:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"user_message\"],\n    template=chat_template,\n)\n\nuser_message = \"Why do I need to run machine learning models on-premise?\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8446/v1\", max_tokens=128)\nchain = LLMChain(llm=chat, prompt=prompt, verbose=True)\nprint(chain.run(user_message=user_message))\n\n```\n\n### \ud83d\udd0e Quality Benchmarks\n\n### \ud83d\udeab Limitations and Biases\n\n## \ud83d\udcdc License\nIt is made available under a permissive Apache 2.0 license allowing for commercial use, without any royalties or restrictions.", "serviceType": "binary", "version": "1", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/binary-only/chat-mistral-7b-openorca/logo.svg", "modelInfo": {"memoryRequirements": 4800}, "interfaces": ["chat"], "defaultPort": 8000, "defaultExternalPort": 8446, "weightsDirectoryUrl": "https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF/resolve/main/", "weightsFiles": ["mistral-7b-openorca.Q5_K_S.gguf"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/premAI-io/prem-services/releases/download/v1/cht-llama-cpp-mistral-1-aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null}, "serveCommand": "cht-llama-cpp-mistral-1-aarch64-apple-darwin --model-path=mistral-7b-openorca.Q5_K_S.gguf --port=8446", "banner": null}]